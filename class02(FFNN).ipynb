{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data is written\n",
      "test data is written\n"
     ]
    }
   ],
   "source": [
    "samples = 1000\n",
    "test_samples = 100\n",
    "train_dataset = './ffnn_dataset/train_dataset.csv'\n",
    "test_dataset = './ffnn_dataset/test_dataset.csv'\n",
    "\n",
    "def write_dataset(samples, test_samples, train_dir, test_dir):\n",
    "    up = [i for i in range(10)]\n",
    "    down = [9-i for i in range(10)]\n",
    "\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(samples):\n",
    "        data.append(up)\n",
    "        data.append(down)\n",
    "        label.append([1])\n",
    "        label.append([0])\n",
    "\n",
    "    with open(train_dataset, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(samples-test_samples):\n",
    "            writer.writerow(label[i] + data[i])\n",
    "        print('train data is written')\n",
    "\n",
    "    with open(test_dataset, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(test_samples):\n",
    "            writer.writerow(label[i] + data[i])\n",
    "        print('test data is written')\n",
    "        \n",
    "#write_dataset(1000, 100, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataset framwork : tf.contrib.data.TextLineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-2156382c08af>:2: TextLineDataset.__init__ (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.TextLineDataset`.\n",
      "<dtype: 'float32'>\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "#decorate, shuffle = seed\n",
    "dataset = tf.contrib.data.TextLineDataset(train_dataset).batch(32)\n",
    "#dataset = dataset.shuffle(7777)\n",
    "#한번 스캔하고 끝내지 말고 반복해서 읽어라 에러내지말고\n",
    "dataset = dataset.repeat(100)\n",
    "\n",
    "itr = dataset.make_one_shot_iterator()\n",
    "\n",
    "batch = itr.get_next()\n",
    "\n",
    "#decoding\n",
    "decoded_batch = tf.decode_csv(batch, record_defaults=[[0]]*11)\n",
    "\n",
    "#rank를 같게 만들어 준다.(reshape, expand_dims 같은 방법)\n",
    "#label = decoded_batch[0]\n",
    "label = tf.reshape(decoded_batch[0], [-1, 1])\n",
    "#label = tf.expand_dims(decoded_batch[0], axis=-1)\n",
    "\n",
    "#column을 사용\n",
    "#row_feature = tf.stack(decoded_batch[1:], axis=0)\n",
    "feature = tf.stack(decoded_batch[1:], axis=1)\n",
    "\n",
    "#타입 변경\n",
    "label = tf.cast(label, tf.float32)\n",
    "feature = tf.cast(feature, tf.float32)\n",
    "\n",
    "print(label.dtype)\n",
    "print(feature.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 shape {} (?, 15)\n",
      "layer2 shape {} (?, 10)\n",
      "layer3 shape {} (?, 5)\n",
      "layer4 shape {} (?, 3)\n",
      "out shape {} (?, 1)\n"
     ]
    }
   ],
   "source": [
    "layer1 = tf.layers.dense(feature, units=15)\n",
    "layer2 = tf.layers.dense(layer1, units=10)\n",
    "layer3 = tf.layers.dense(layer2, units=5)\n",
    "layer4 = tf.layers.dense(layer3, units=3)\n",
    "out = tf.layers.dense(layer4, units=1)\n",
    "\n",
    "print('layer1 shape {}',format(layer1.shape))\n",
    "print('layer2 shape {}',format(layer2.shape))\n",
    "print('layer3 shape {}',format(layer3.shape))\n",
    "print('layer4 shape {}',format(layer4.shape))\n",
    "print('out shape {}',format(out.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- label 이랑 out이랑 shape이 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.sigmoid_cross_entropy(label, out)\n",
    "train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.0552276112139225\n",
      "step: 100, loss: 0.0020712092518806458\n",
      "step: 200, loss: 0.000978351105004549\n",
      "step: 300, loss: 0.0006241615628823638\n",
      "step: 400, loss: 0.00045241316547617316\n",
      "step: 500, loss: 0.0003520671743899584\n",
      "step: 600, loss: 0.00028668655431829393\n",
      "step: 700, loss: 0.00024090615625027567\n",
      "step: 800, loss: 0.0002071705530397594\n",
      "step: 900, loss: 0.0001813407288864255\n",
      "step: 1000, loss: 0.00016096752369776368\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    _loss, _ = sess.run([loss, train_op])\n",
    "    for i in range(1001):\n",
    "        _loss, _ = sess.run([loss, train_op])\n",
    "        if i%100 == 0:\n",
    "            print('step: {}, loss: {}'.format(i, _loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data is written\n",
      "test data is written\n"
     ]
    }
   ],
   "source": [
    "samples = 1000\n",
    "test_samples = 100\n",
    "train_dataset = './ffnn_dataset/train_dataset.csv'\n",
    "test_dataset = './ffnn_dataset/test_dataset.csv'\n",
    "\n",
    "def write_dataset(samples, test_samples, train_dir, test_dir):\n",
    "    up = [i for i in range(10)]\n",
    "    down = [9-i for i in range(10)]\n",
    "\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(samples):\n",
    "        data.append(up)\n",
    "        data.append(down)\n",
    "        label.append([1])\n",
    "        label.append([0])\n",
    "\n",
    "    with open(train_dataset, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(samples-test_samples):\n",
    "            writer.writerow(label[i] + data[i])\n",
    "        print('train data is written')\n",
    "\n",
    "    with open(test_dataset, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(test_samples):\n",
    "            writer.writerow(label[i] + data[i])\n",
    "        print('test data is written')\n",
    "        \n",
    "write_dataset(1000, 100, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-08e1cc7ab86e>:1: TextLineDataset.__init__ (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.TextLineDataset`.\n"
     ]
    }
   ],
   "source": [
    "trainset = tf.contrib.data.TextLineDataset(train_dataset).batch(10)\n",
    "testset = tf.contrib.data.TextLineDataset(test_dataset).batch(10)\n",
    "\n",
    "trainset = trainset.repeat(100)\n",
    "testset = testset.repeat(999999)\n",
    "\n",
    "train_itr = trainset.make_one_shot_iterator()\n",
    "test_itr = testset.make_one_shot_iterator()\n",
    "\n",
    "train_batch = train_itr.get_next()\n",
    "test_batch = test_itr.get_next()\n",
    "\n",
    "train_batch = tf.decode_csv(train_batch, [[0]]*11)\n",
    "test_batch = tf.decode_csv(test_batch, [[0]]*11)\n",
    "\n",
    "train_itr = trainset.make_one_shot_iterator()\n",
    "test_itr = testset.make_one_shot_iterator()\n",
    "\n",
    "train_batch = train_itr.get_next()\n",
    "test_batch = test_itr.get_next()\n",
    "\n",
    "train_decoded_batch = tf.decode_csv(train_batch, record_defaults=[[0]]*11)\n",
    "test_decoded_batch = tf.decode_csv(test_batch, record_defaults=[[0]]*11)\n",
    "\n",
    "train_label = tf.reshape(train_decoded_batch[0], [-1, 1])\n",
    "test_label = tf.reshape(test_decoded_batch[0], [-1, 1])\n",
    "\n",
    "train_feature = tf.stack(train_decoded_batch[1:], axis=1)\n",
    "test_feature = tf.stack(test_decoded_batch[1:], axis=1)\n",
    "\n",
    "\n",
    "train_label = tf.cast(train_label, tf.float32)\n",
    "train_feature = tf.cast(train_feature, tf.float32)\n",
    "\n",
    "test_label = tf.cast(test_label, tf.float32)\n",
    "test_feature = tf.cast(test_feature, tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 shape {} (?, 15)\n",
      "layer2 shape {} (?, 10)\n",
      "layer3 shape {} (?, 5)\n",
      "layer4 shape {} (?, 3)\n",
      "out shape {} (?, 1)\n",
      "layer1 shape {} (?, 15)\n",
      "layer2 shape {} (?, 10)\n",
      "layer3 shape {} (?, 5)\n",
      "layer4 shape {} (?, 3)\n",
      "out shape {} (?, 1)\n",
      "Tensor(\"out/BiasAdd:0\", shape=(?, 1), dtype=float32) Tensor(\"out_2/BiasAdd:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def bin_model(feature, activation=None, reuse=False):\n",
    "    layer1 = tf.layers.dense(feature, units=15, activation=activation, reuse=reuse, name='layer1')\n",
    "    layer2 = tf.layers.dense(layer1, units=10, activation=activation, reuse=reuse, name='layer2')\n",
    "    layer3 = tf.layers.dense(layer2, units=5, activation=activation, reuse=reuse, name='layer3')\n",
    "    layer4 = tf.layers.dense(layer3, units=3, activation=activation, reuse=reuse, name='layer4')\n",
    "    out = tf.layers.dense(layer4, units=1, reuse=reuse, name='out')\n",
    "    print('layer1 shape {}',format(layer1.shape))\n",
    "    print('layer2 shape {}',format(layer2.shape))\n",
    "    print('layer3 shape {}',format(layer3.shape))\n",
    "    print('layer4 shape {}',format(layer4.shape))\n",
    "    print('out shape {}',format(out.shape))\n",
    "    return out\n",
    "\n",
    "\n",
    "train_out = bin_model(train_feature)\n",
    "#reuse=True = 기존모델 그대로 사용해라\n",
    "test_out = bin_model(test_feature, reuse=True)\n",
    "print(train_out, test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'layer1/kernel:0' shape=(10, 15) dtype=float32_ref>\n",
      "<tf.Variable 'layer1/bias:0' shape=(15,) dtype=float32_ref>\n",
      "<tf.Variable 'layer2/kernel:0' shape=(15, 10) dtype=float32_ref>\n",
      "<tf.Variable 'layer2/bias:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'layer3/kernel:0' shape=(10, 5) dtype=float32_ref>\n",
      "<tf.Variable 'layer3/bias:0' shape=(5,) dtype=float32_ref>\n",
      "<tf.Variable 'layer4/kernel:0' shape=(5, 3) dtype=float32_ref>\n",
      "<tf.Variable 'layer4/bias:0' shape=(3,) dtype=float32_ref>\n",
      "<tf.Variable 'out/kernel:0' shape=(3, 1) dtype=float32_ref>\n",
      "<tf.Variable 'out/bias:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "for v in tf.trainable_variables():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#모델 세이브 saver()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.sigmoid_cross_entropy(train_label, train_out)\n",
    "train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)\n",
    "\n",
    "pred = tf.nn.sigmoid(test_out)\n",
    "accuracy = tf.metrics.accuracy(test_label, tf.round(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#graph 그리기\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.0072141303680837154, acc: 0.0\n",
      "step: 100, loss: 0.0003634498571045697, acc: 1.0\n",
      "step: 200, loss: 0.00020566396415233612, acc: 1.0\n",
      "step: 300, loss: 0.00014801087672822177, acc: 1.0\n",
      "step: 400, loss: 0.00011711772822309285, acc: 1.0\n",
      "step: 500, loss: 9.749164746608585e-05, acc: 1.0\n",
      "step: 600, loss: 8.376597543247044e-05, acc: 1.0\n",
      "step: 700, loss: 7.355816342169419e-05, acc: 1.0\n",
      "step: 800, loss: 6.563468923559412e-05, acc: 1.0\n",
      "step: 900, loss: 5.928733662585728e-05, acc: 1.0\n",
      "step: 1000, loss: 5.4078391258371994e-05, acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())#accuracy 계속 이동 평균구하기 때문, 내부적인 variable 필요\n",
    "    writer = tf.summary.FileWriter('./ffnn_logs/', sess.graph)\n",
    "    for i in range(1001):\n",
    "        _, _loss, _acc, _summ = sess.run([train_op, loss, accuracy, merged])\n",
    "        writer.add_summary(_summ, i)\n",
    "        if i%100 == 0:\n",
    "            _pred = sess.run(pred)\n",
    "            print('step: {}, loss: {}, acc: {}'.format(i, _loss, _acc[0]))\n",
    "            \n",
    "    saver.save(sess, './ffnn_logs/ffnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './ffnn_logs/ffnn')\n",
    "    _pred = sess.run(pred)\n",
    "    print(_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph 확인\n",
    "- $tensorboard --logdir=./ffnn_logs\n",
    "- http://0.0.0.0:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
